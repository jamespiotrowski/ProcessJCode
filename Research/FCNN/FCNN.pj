import std.*;

/**********************************************
 * layerSegmentComponent
 * 
 *      record to store parameters needed
 *      for a layer segment
**********************************************/
record layerSegmentComponent{
    int minNode                         // first node to compute
    , int maxNode                       // last node to compute
    , int nextLayerSize                 // Number of nodes to write to
}

/**********************************************
 * xavierInitialization (FUNCTION)
 * 
 *      init function for neural network
 *      weights to help prevent vanishing
 *      gradient.
 * 
 *      Reference: https://www.youtube.com/watch?v=8krd5qKVw-Q&t=352s
**********************************************/
double xavierInitialization(int numInputs){
    return gaussianRandom() * sqrt(1.0 / (double)numInputs);
}

/**********************************************
 * zeroOutArray (FUNCTION)
 * 
 *      function to quickly reset an array
 *      needed for the result array (s) in
 *      neural networks each time a new 
 *      input is computed
**********************************************/
void zeroOutArray(int minIndex, int maxIndex, double[] arr){
    for(int i = minIndex; i < maxIndex; i++){
        arr[i] = 0.0;
    }
}

/**********************************************
 * layerSegment (PJ PROCESS)
 * 
 *      A layer segment does not spawn 
 *      additional PJ processes. Neural
 *      networks can create a ton of 
 *      overhead, so the granularity
 *      ends with a layer segment. 
**********************************************/
void layerSegment(
    layerSegmentComponent lsc
    , chan<double>[][] inputChannels    // channels to read input from
    , double[][] w                      // weights
    , double[] b                        // bias
    , double[] s                        // storage for summation
    , double[] y                        // output for channels
    , chan<double>[] outputChannls      // Channel to write out... needs to write nextLayerSize times
){
    // Right now this only reads one input then quits. 
    // Need infinite loop with kill... or number of times
    // passed in the lsc
    double i;
    zeroOutArray(lsc.minNode, lsc.maxNode, s);
    alt(int node = lsc.minNode; node < lsc.maxNode; node++){
        alt(int input = 0; input < lsc.inputChannels.size; input++){
            i = inputChannels[input][node].read() : {
                s[node] = lsc.s[n]
            }
        }
    }
}

void FullyConnectedNeuralNetwork(const int inputSize, const int[] layerSizes){
    print("<FullyConnectedNeuralNetwork>: Creating Neural Network with input size: {" + inputSize + "} and shape: { ");
    for(int i = 0; i < layerSizes.size; i++){
        print(layerSizes[i] + " ");
    }
    println("}");

    /***************************** 
     * FCNN Stats
    *****************************/
    println("<FullyConnectedNeuralNetwork>: Creating network skeleton...");
    int numLayers = layerSizes.size;

    println("    - <FullyConnectedNeuralNetwork>: Allocating channels & weights...");
    chan<double>[][][] connections = new chan<double>[numLayers][0][0];
    double[][][] w = new double[numLayers][0][0];
    double[][][] wN = new double[numLayers][0][0];
    double[][] b = new double[numLayers][0];
    double[][] bN = new double[numLayers][0];
    double[][] s = new double[numLayers][0];
    double[][] y = new double[numLayers][0];
    double[][] errorStore = new double[numLayers][0];
    double[] error = new double[layerSizes[numLayers - 1]];

    for(int i = 0; i < numLayers; i++){
        // Dimensions
        int lowerSize = (i == 0) ? inputSize : layerSizes[i - 1];
        int upperSize = layerSizes[i];
        // Allocations
        connections[i] = new chan<double>[lowerSize][upperSize];
        w[i] = new double[lowerSize][upperSize];
        wN[i] = new double[lowerSize][upperSize];
        b[i] = new double[upperSize];
        bN[i] = new double[upperSize];
        s[i] = new double[upperSize];
        y[i] = new double[upperSize];
        errorStore[i] = new double[upperSize];
        for(int j = 0; j < upperSize; j++){
            b[i][j] = xavierInitialization(lowerSize);
            bN[i][j] = b[i][j];
            s[i][j] = 0;
            y[i][j] = 0;
            errorStore[i][j] = 0;
            for(int k = 0; k < lowerSize; k++){
                w[i][k][j] = xavierInitialization(lowerSize);
                wN[i][k][j] = w[i][k][j];
            }   
        }
    }

    println("    - <FullyConnectedNeuralNetwork>: Checking allocations");
    println("        - <FullyConnectedNeuralNetwork>: connections: ");
    for(int i = 0; i < connections.size; i++){
        println("            - Layer " + i + ": { " + connections[i].size + " , " + connections[i][0].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: w: ");
    for(int i = 0; i < w.size; i++){
        println("            - Layer " + i + ": { " + w[i].size + " , " + w[i][0].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: wN: ");
    for(int i = 0; i < wN.size; i++){
        println("            - Layer " + i + ": { " + wN[i].size + " , " + wN[i][0].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: b: ");
    for(int i = 0; i < b.size; i++){
        println("            - Layer " + i + ": { " + b[i].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: bN: ");
    for(int i = 0; i < bN.size; i++){
        println("            - Layer " + i + ": { " + bN[i].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: s: ");
    for(int i = 0; i < s.size; i++){
        println("            - Layer " + i + ": { " + s[i].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: y: ");
    for(int i = 0; i < y.size; i++){
        println("            - Layer " + i + ": { " + y[i].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: errorStore: ");
    for(int i = 0; i < errorStore.size; i++){
        println("            - Layer " + i + ": { " + errorStore[i].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: error: ");
    println("            - { " + error.size + " }");
}

public void main(string[] args){
   

    int inputSize = 728;
    int numLayers = 3;
    
    int[] layers = new int[numLayers];
    layers[0] = 40; layers[1] = 20; layers[2] = 10;

    FullyConnectedNeuralNetwork(inputSize, layers);

}



/*
 * First, initialize weights with value that is (centered around value 0 with variance 1)
 * mean is 0, standard deviation is 1
 * https://www.socscistatistics.com/utilities/normaldistribution/default.aspx
 * https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php
 * 
 * then, multiply value by sqrt(1/n) where n is the number of inputs to the node
 * so, for a node in the first layer, it would be like (-0.2) * sqrt(1 / 728)
 * 
 * 
 * 
 * https://levelup.gitconnected.com/how-do-computers-generate-random-numbers-a72be65877f6
 * make a library for random numbers
 */
import std.*;

// Make final eventually?
public const int SIGMOID = 0;
public const int TANH = 1;
public const int LEAKYRELU = 2;
public const int SOFTMAX = 3;
public const int RELU = 4;

/**********************************************
 * layerSegmentComponent
 * 
 *      record to store parameters needed
 *      for a layer segment
**********************************************/
public record layerSegmentComponent{
    int minNode;                       // first node to compute
    int maxNode;                       // last node to compute
    int nextLayerSize;                 // Number of nodes to write to
    int layerId;
}

/**********************************************
 * xavierInitialization (FUNCTION)
 * 
 *      init function for neural network
 *      weights to help prevent vanishing
 *      gradient.
 * 
 *      Reference: https://www.youtube.com/watch?v=8krd5qKVw-Q&t=352s
**********************************************/
double xavierInitialization(int numInputs){
    return gaussianRandom() * sqrt(1.0 / (double)numInputs);
}

/**********************************************
 * zeroOutArray (FUNCTION)
 * 
 *      function to quickly reset an array
 *      needed for the result array (s) in
 *      neural networks each time a new 
 *      input is computed
**********************************************/
void zeroOutArray(int minIndex, int maxIndex, double[] arr){
    for(int i = minIndex; i < maxIndex; i++){
        arr[i] = 0.0;
    }
}

/**********************************************
 * initializeArrayWithArray (FUNCTION)
 * 
 *      function to quickly reset an array
 *      needed for the result array (s) in
 *      neural networks each time a new 
 *      input is computed
**********************************************/
void initializeArrayWithArray(int minIndex, int maxIndex, double[] arr, double[] vals){
    for(int i = minIndex; i < maxIndex; i++){
        arr[i] = vals[i];
    }
}

/**********************************************
 * sigmoid (FUNCTION)
 * 
 *      sigmoid function
 *      static double Sigmoid(double d) { return (1.0 / (1.0 + exp(-d))); }
**********************************************/
double sigmoid(double d){
    return (1.0 / (1.0 + exp(-d)));
}

/**********************************************
 * activationFunction (FUNCTION)
 * 
 *      
**********************************************/
double activationFunction(double d, int af){
    switch(af){
        case(0) : { // Replace with SIGMOID if possible
            return sigmoid(d);
        }
        default : {
            //println("ERROR <activationFunction>: Activation Functions not selected.");
            return 0.0;
        }
    }
}

/**********************************************
 * activationFunctionDerivative (FUNCTION)
 * 
 *      
**********************************************/
double activationFunctionDerivative(double d, int af){
    switch(af){
        case(0) : { // Replace with SIGMOID if possible
            return (d * (1.0 - d));
        }
        default : {
            //println("ERROR <activationFunction>: Activation Functions not selected.");
            return 0.0;
        }
    }
}

/**********************************************
 * inputLayerSegment (PJ PROCESS)
 * 
**********************************************/

/**********************************************
 * outputLayerSegment (PJ PROCESS)
 * 
**********************************************/

/**********************************************
 * hiddenLayerSegment (PJ PROCESS)
 * 
 *      A layer segment does not spawn 
 *      additional PJ processes. Neural
 *      networks can create a ton of 
 *      overhead, so the granularity
 *      ends with a layer segment. 
**********************************************/
void hiddenLayerSegment(
    layerSegmentComponent lsc
    , chan<double>.read[][] inputChannels       // channels to read input from... read previousLayerSize times
    , chan<double>.write[][] outputChannels     // Channel to write out... needs to write nextLayerSize times
    , chan<double>.read[] errorInChannels    // Channel to recieve error for bp ... needs to read nextLayerSize times
    , chan<double>.write[] errorOutChannels   // Channel to write error for bp ... needs to write previousLayerSize times
    , double[][] w                      // weights
    , double[][] wN                     // weights
    , double[] b                        // bias
    , double[] bN                       // bias
    , double[] s                        // storage for summation
    , double[] y                        // output for channels
    , double[] errorStore
    , int af
    , chan<boolean>.read bpChannel      // Channel used to turn off back prop (or turn it on...) - perhaps use this for every input :) (for now)
    , chan<boolean>.read killChannel    // Kill channel. uhhhh. Only one layer needs to write. Need layer id
){
    boolean backProp;
    boolean kill = false;
    while(true){
        // Determine if to take input or die
        pri alt{
            backProp = bpChannel.read() : {  }
            kill = killChannel.read() : {
                if(kill){
                    break; // or break?
                }
                else{
                    continue; // is this in pj?
                }
            }
        }
        
        /**********************************/
        /* FORWARD COMPUTATION            */
        /**********************************/
        double inp;
        initializeArrayWithArray(lsc.minNode, lsc.maxNode, s, b); // Initializes all of s with bias values 
        // NEED A FOR LOOP OUT HERE
        alt(int node = lsc.minNode; node < lsc.maxNode; node++){
            alt(int input = 0; input < inputChannels.size; input++){
                inp = inputChannels[input][node].read() : {
                    s[node] = s[node] + inp;
                }
            }
        }
        // Now we can compute activation function and write.
        /* Write to next node.. this is complicated.
            TODO: Determine if should be done with fors or pars (... or alts? Dont think alt to alt is possible)
                Next layer will be waiting on ALTs... so for loop should be fine. 
                Pars might be better, but I think too much overhead because each becomes PJ process
        */
        for(int node = lsc.minNode; node < lsc.maxNode; node++){
            y[node] = activationFunction(s[node], af);
            // for... par... alt???
            for(int nextNode = 0; nextNode < lsc.nextLayerSize; nextNode++){
                outputChannels[node][nextNode].write(y[node] * w[node][nextNode]);
            }
        }

        /**********************************/
        /* BACKWARD COMPUTATION           */
        /**********************************/
        
        if(backProp){
            // First we must read in errors and compute
            double err;
            zeroOutArray(lsc.minNode, lsc.maxNode, errorStore); // Initializes all with 0
            // For each node in next layer
            // NEED A FOR LOOP ON OUTSIDE OF REPLICATED ALT
            alt(int nextNode = 0; nextNode < lsc.nextLayerSize; nextNode++){
                // Read the error
                err = errorInChannels[nextNode].read() : {
                    // Then for each node in the layer segment
                    for(int node = lsc.minNode; node < lsc.maxNode; node++){
                        wN[node][nextNode] = w[node][nextNode] - (err * y[node]);
                        errorStore[node] = errorStore[node] + (err * w[node][nextNode]);
                    }
                }
            }
            // Update bias and write out
            for(int node = lsc.minNode; node < lsc.maxNode; node++){
                errorStore[node] = errorStore[node] * activationFunctionDerivative(y[node], af);
                bN[node] = b[node] - errorStore[node];
                errorOutChannels[node].write(errorStore[node]);
            }
        }
         
    }
}

void FullyConnectedNeuralNetwork(const int inputSize, const int[] layerSizes, int af){
    print("<FullyConnectedNeuralNetwork>: Creating Neural Network with input size: {" + inputSize + "} and shape: { ");
    for(int i = 0; i < layerSizes.size; i++){
        print(layerSizes[i] + " ");
    }
    println("}");

    /***************************** 
     * FCNN Stats
    *****************************/
    println("<FullyConnectedNeuralNetwork>: Creating network skeleton...");
    int numLayers = layerSizes.size;

    println("    - <FullyConnectedNeuralNetwork>: Allocating channels & weights...");
    chan<double>[][][] connections = new chan<double>[numLayers][0][0];
    double[][][] w = new double[numLayers][0][0];
    double[][][] wN = new double[numLayers][0][0];
    double[][] b = new double[numLayers][0];
    double[][] bN = new double[numLayers][0];
    double[][] s = new double[numLayers][0];
    double[][] y = new double[numLayers][0];
    double[][] errorStore = new double[numLayers][0];
    double[] error = new double[layerSizes[numLayers - 1]];

    for(int i = 0; i < numLayers; i++){
        // Dimensions
        int lowerSize = (i == 0) ? inputSize : layerSizes[i - 1];
        int upperSize = layerSizes[i];
        // Allocations
        connections[i] = new chan<double>[lowerSize][upperSize];
        w[i] = new double[lowerSize][upperSize];
        wN[i] = new double[lowerSize][upperSize];
        b[i] = new double[upperSize];
        bN[i] = new double[upperSize];
        s[i] = new double[upperSize];
        y[i] = new double[upperSize];
        errorStore[i] = new double[upperSize];
        for(int j = 0; j < upperSize; j++){
            b[i][j] = xavierInitialization(lowerSize);
            bN[i][j] = b[i][j];
            s[i][j] = 0;
            y[i][j] = 0;
            errorStore[i][j] = 0;
            for(int k = 0; k < lowerSize; k++){
                w[i][k][j] = xavierInitialization(lowerSize);
                wN[i][k][j] = w[i][k][j];
            }   
        }
    }

    println("    - <FullyConnectedNeuralNetwork>: Checking allocations");
    println("        - <FullyConnectedNeuralNetwork>: connections: ");
    for(int i = 0; i < connections.size; i++){
        println("            - Layer " + i + ": { " + connections[i].size + " , " + connections[i][0].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: w: ");
    for(int i = 0; i < w.size; i++){
        println("            - Layer " + i + ": { " + w[i].size + " , " + w[i][0].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: wN: ");
    for(int i = 0; i < wN.size; i++){
        println("            - Layer " + i + ": { " + wN[i].size + " , " + wN[i][0].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: b: ");
    for(int i = 0; i < b.size; i++){
        println("            - Layer " + i + ": { " + b[i].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: bN: ");
    for(int i = 0; i < bN.size; i++){
        println("            - Layer " + i + ": { " + bN[i].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: s: ");
    for(int i = 0; i < s.size; i++){
        println("            - Layer " + i + ": { " + s[i].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: y: ");
    for(int i = 0; i < y.size; i++){
        println("            - Layer " + i + ": { " + y[i].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: errorStore: ");
    for(int i = 0; i < errorStore.size; i++){
        println("            - Layer " + i + ": { " + errorStore[i].size + " }");
    }

    println("        - <FullyConnectedNeuralNetwork>: error: ");
    println("            - { " + error.size + " }");
}

public void main(string[] args){
   

    int inputSize = 728;
    int numLayers = 3;
    
    int[] layers = new int[numLayers];
    layers[0] = 40; layers[1] = 20; layers[2] = 10;

    FullyConnectedNeuralNetwork(inputSize, layers, SIGMOID);

}



/*
 * First, initialize weights with value that is (centered around value 0 with variance 1)
 * mean is 0, standard deviation is 1
 * https://www.socscistatistics.com/utilities/normaldistribution/default.aspx
 * https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php
 * 
 * then, multiply value by sqrt(1/n) where n is the number of inputs to the node
 * so, for a node in the first layer, it would be like (-0.2) * sqrt(1 / 728)
 * 
 * 
 * 
 * https://levelup.gitconnected.com/how-do-computers-generate-random-numbers-a72be65877f6
 * make a library for random numbers
 */